{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8577991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "import transformers\n",
    "import contractions\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW, TFAutoModelForSequenceClassification\n",
    "import gc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b78b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43410, 29)\n",
      "(5426, 29)\n",
      "(5427, 29)\n"
     ]
    }
   ],
   "source": [
    "# Importing train, validation and test datasets with preprocessed texts and labels\n",
    "train_GE = pd.read_csv(\"train_clean.csv\")\n",
    "val_GE = pd.read_csv(\"val_clean.csv\")\n",
    "test_GE = pd.read_csv(\"test_clean.csv\")\n",
    "\n",
    "# Shape validation\n",
    "print(train_GE.shape)\n",
    "print(val_GE.shape)\n",
    "print(test_GE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55471e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading emotion labels for GoEmotions taxonomy\n",
    "with open(\"emotions.txt\", \"r\") as file:\n",
    "    GE_taxonomy = file.read().split(\"\\n\")\n",
    "\n",
    "GE_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8270e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_taxonomy_no_neu = GE_taxonomy.copy()\n",
    "GE_taxonomy_no_neu.remove('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103de10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GE_taxonomy_no_neu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42e1f4",
   "metadata": {},
   "source": [
    "#### Creating GoEmotion excluding neutral emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fed278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_GE_no_neu = train_GE.copy()\n",
    "df_val_GE_no_neu = val_GE.copy()\n",
    "df_test_GE_no_neu = test_GE.copy()\n",
    "\n",
    "df_train_GE_no_neu = df_train_GE_no_neu.drop(columns=['neutral'])\n",
    "df_val_GE_no_neu = df_val_GE_no_neu.drop(columns=['neutral'])\n",
    "df_test_GE_no_neu = df_test_GE_no_neu.drop(columns=['neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b57a69",
   "metadata": {},
   "source": [
    "Then, we need remove all the samples that have been left without a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac5598d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30587, 28)\n",
      "(3834, 28)\n",
      "(3821, 28)\n"
     ]
    }
   ],
   "source": [
    "# Removing samples with only 0 in their labels\n",
    "df_train_GE_no_neu = df_train_GE_no_neu.loc[ df_train_GE_no_neu.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n",
    "df_val_GE_no_neu = df_val_GE_no_neu.loc[ df_val_GE_no_neu.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n",
    "df_test_GE_no_neu = df_test_GE_no_neu.loc[ df_test_GE_no_neu.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n",
    "\n",
    "# Shape validation\n",
    "print(df_train_GE_no_neu.shape)\n",
    "print(df_val_GE_no_neu.shape)\n",
    "print(df_test_GE_no_neu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35ae1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>disapproval</th>\n",
       "      <th>disgust</th>\n",
       "      <th>embarrassment</th>\n",
       "      <th>excitement</th>\n",
       "      <th>fear</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>grief</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Clean_text  admiration  amusement  anger  annoyance  \\\n",
       "2  why the fuck is bayless isoing           0          0      1          0   \n",
       "3     to make her feel threatened           0          0      0          0   \n",
       "4          dirty southern wankers           0          0      0          1   \n",
       "\n",
       "   approval  caring  confusion  curiosity  desire  disappointment  \\\n",
       "2         0       0          0          0       0               0   \n",
       "3         0       0          0          0       0               0   \n",
       "4         0       0          0          0       0               0   \n",
       "\n",
       "   disapproval  disgust  embarrassment  excitement  fear  gratitude  grief  \\\n",
       "2            0        0              0           0     0          0      0   \n",
       "3            0        0              0           0     1          0      0   \n",
       "4            0        0              0           0     0          0      0   \n",
       "\n",
       "   joy  love  nervousness  optimism  pride  realization  relief  remorse  \\\n",
       "2    0     0            0         0      0            0       0        0   \n",
       "3    0     0            0         0      0            0       0        0   \n",
       "4    0     0            0         0      0            0       0        0   \n",
       "\n",
       "   sadness  surprise  \n",
       "2        0         0  \n",
       "3        0         0  \n",
       "4        0         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_GE_no_neu.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5e5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43410,) (43410, 28) (5426,) (5426, 28) (5427,) (5427, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59386e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from probabilities to labels using a given threshold\n",
    "def proba_to_labels(y_pred_proba, threshold=0.8):\n",
    "    \n",
    "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
    "    \n",
    "    for i in range(y_pred_proba.shape[0]):\n",
    "        for j in range(y_pred_proba.shape[1]):\n",
    "            if y_pred_proba[i][j] > threshold:\n",
    "                y_pred_labels[i][j] = 1\n",
    "            else:\n",
    "                y_pred_labels[i][j] = 0\n",
    "                \n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ea228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(predictions):\n",
    "    arr =[]\n",
    "    for item in predictions:\n",
    "        prob = []\n",
    "        for tup in item:\n",
    "            prob.append(tup[1])\n",
    "        arr.append(prob)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb8c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function \n",
    "def model_eval(y_true, y_pred_labels, emotions):\n",
    "    \n",
    "    # Defining variables\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    # Per emotion evaluation      \n",
    "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
    "    \n",
    "    for i in range(len(emotions)):\n",
    "   \n",
    "        # Computing precision, recall and f1-score\n",
    "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
    "        \n",
    "        # Append results in lists\n",
    "        precision.append(round(p, 2))\n",
    "        recall.append(round(r, 2))\n",
    "        f1.append(round(f1_score, 2))\n",
    "    \n",
    "    # Macro evaluation\n",
    "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
    "    \n",
    "    # Append results in lists\n",
    "    precision.append(round(macro_p, 2))\n",
    "    recall.append(round(macro_r, 2))\n",
    "    f1.append(round(macro_f1_score, 2))\n",
    "    \n",
    "    # Converting results to a dataframe\n",
    "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
    "    df_results.index = emotions+['MACRO-AVERAGE']\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48223f50",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c6e702",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f9271",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e380072f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbd7e3",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb855a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64f3370",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc3f86",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28eaee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e757882",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae52e2d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a6771",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7b0a5",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bc5e9",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934b0282",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/4\n",
      "7235/7235 [==============================] - 628s 85ms/step - loss: 0.1375 - accuracy: 0.4258 - val_loss: 0.0942 - val_accuracy: 0.5459\n",
      "Epoch 2/4\n",
      "7235/7235 [==============================] - 610s 84ms/step - loss: 0.0914 - accuracy: 0.5498 - val_loss: 0.0887 - val_accuracy: 0.5494\n",
      "Epoch 3/4\n",
      "7235/7235 [==============================] - 613s 84ms/step - loss: 0.0812 - accuracy: 0.5907 - val_loss: 0.0850 - val_accuracy: 0.5540\n",
      "Epoch 4/4\n",
      "7235/7235 [==============================] - 606s 83ms/step - loss: 0.0668 - accuracy: 0.6623 - val_loss: 0.0837 - val_accuracy: 0.5724\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on Emotion Dataset: \n",
      " 40.941396965\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=3e-5, epochs=4)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaef8ae",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7748f3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7804d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e1d611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:4880 | loss:0.6 | true:[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.01 0.02 0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.95])\n",
      "\n",
      "----------\n",
      "id:3527 | loss:0.58 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.01 0.02 0.   0.04 0.11 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.01 0.   0.02 0.   0.   0.   0.01 0.81])\n",
      "\n",
      "----------\n",
      "id:1433 | loss:0.57 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.01 0.03 0.02 0.   0.   0.   0.   0.01 0.03 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.9 ])\n",
      "\n",
      "----------\n",
      "id:450 | loss:0.53 | true:[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.01 0.   0.01 0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.94])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2aceca",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52836e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a67b1",
   "metadata": {},
   "source": [
    "### LR - 3e-5, batch size = 6, epochs = 4, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e005ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d052170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_2 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-1')\n",
    "roberta_predictor_2.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa75585",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_2 = roberta_predictor_2.predict(X_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ca1b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from probabilities to labels using a given threshold\n",
    "def proba_to_labels(y_pred_proba, threshold=0.8):\n",
    "    \n",
    "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
    "    \n",
    "    for i in range(y_pred_proba.shape[0]):\n",
    "        for j in range(y_pred_proba.shape[1]):\n",
    "            if y_pred_proba[i][j] > threshold:\n",
    "                y_pred_labels[i][j] = 1\n",
    "            else:\n",
    "                y_pred_labels[i][j] = 0\n",
    "                \n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df74c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(predictions):\n",
    "    arr =[]\n",
    "    for item in predictions:\n",
    "        prob = []\n",
    "        for tup in item:\n",
    "            prob.append(tup[1])\n",
    "        arr.append(prob)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09b824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function \n",
    "def model_eval(y_true, y_pred_labels, emotions):\n",
    "    \n",
    "    # Defining variables\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    # Per emotion evaluation      \n",
    "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
    "    \n",
    "    for i in range(len(emotions)):\n",
    "   \n",
    "        # Computing precision, recall and f1-score\n",
    "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
    "        \n",
    "        # Append results in lists\n",
    "        precision.append(round(p, 2))\n",
    "        recall.append(round(r, 2))\n",
    "        f1.append(round(f1_score, 2))\n",
    "    \n",
    "    # Macro evaluation\n",
    "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
    "    \n",
    "    # Append results in lists\n",
    "    precision.append(round(macro_p, 2))\n",
    "    recall.append(round(macro_r, 2))\n",
    "    f1.append(round(macro_f1_score, 2))\n",
    "    \n",
    "    # Converting results to a dataframe\n",
    "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
    "    df_results.index = emotions+['MACRO-AVERAGE']\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db5040ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7903112e-02, 8.7302392e-03, 7.7303494e-03, ..., 1.8732747e-01,\n",
       "        2.5483682e-03, 1.1553922e-02],\n",
       "       [7.5364554e-01, 5.3642043e-03, 1.6193381e-02, ..., 5.3660641e-03,\n",
       "        1.1223877e-02, 8.1870584e-03],\n",
       "       [3.1769121e-01, 1.1598411e-02, 3.0920238e-03, ..., 7.6985470e-04,\n",
       "        6.0549811e-03, 9.3656573e-03],\n",
       "       ...,\n",
       "       [3.0389531e-03, 2.1732687e-03, 7.2334553e-03, ..., 3.0056757e-03,\n",
       "        1.2960446e-03, 9.6281958e-01],\n",
       "       [7.3077941e-01, 5.6156782e-03, 9.4018033e-04, ..., 1.9416597e-03,\n",
       "        4.5370250e-03, 5.3617670e-03],\n",
       "       [4.3817065e-03, 6.7902119e-03, 9.7010576e-04, ..., 2.5577059e-03,\n",
       "        1.8391563e-03, 9.2432725e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba_2 = get_proba(test_predictions_2)\n",
    "test_proba_2 = np.array(test_proba_2)\n",
    "test_proba_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2865cd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_2 = proba_to_labels(test_proba_2)\n",
    "y_pred_labels_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "422f4dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.78    0.56  0.65\n",
       "amusement            0.84    0.62  0.71\n",
       "anger                0.67    0.15  0.24\n",
       "annoyance            1.00    0.02  0.05\n",
       "approval             0.79    0.17  0.28\n",
       "caring               0.63    0.19  0.30\n",
       "confusion            0.68    0.21  0.32\n",
       "curiosity            0.77    0.06  0.11\n",
       "desire               0.71    0.18  0.29\n",
       "disappointment       0.88    0.05  0.09\n",
       "disapproval          0.68    0.10  0.18\n",
       "disgust              0.81    0.20  0.32\n",
       "embarrassment        0.80    0.22  0.34\n",
       "excitement           0.77    0.22  0.35\n",
       "fear                 0.78    0.46  0.58\n",
       "gratitude            0.96    0.88  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.82    0.25  0.38\n",
       "love                 0.88    0.65  0.74\n",
       "nervousness          0.00    0.00  0.00\n",
       "optimism             0.75    0.27  0.40\n",
       "pride                0.00    0.00  0.00\n",
       "realization          0.82    0.06  0.12\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.17    0.02  0.03\n",
       "sadness              0.86    0.42  0.56\n",
       "surprise             0.73    0.29  0.42\n",
       "neutral              0.80    0.30  0.44\n",
       "MACRO-AVERAGE        0.66    0.23  0.31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_2, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade81e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582a173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db81c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e57543",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7353b9f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=48, classes=GE_taxonomy, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecf2fe",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02786680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aeae45",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c0f10a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514f45b5",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f7e62",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13900e85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3bfddb",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9f6856",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867731a9",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f7e14",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a92a9",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c979a11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/10\n",
      "4341/4341 [==============================] - 429s 95ms/step - loss: 0.1429 - accuracy: 0.4140 - val_loss: 0.0982 - val_accuracy: 0.5311\n",
      "Epoch 2/10\n",
      "4341/4341 [==============================] - 418s 95ms/step - loss: 0.0930 - accuracy: 0.5523 - val_loss: 0.0881 - val_accuracy: 0.5612\n",
      "Epoch 3/10\n",
      "4341/4341 [==============================] - 418s 95ms/step - loss: 0.0842 - accuracy: 0.5824 - val_loss: 0.0893 - val_accuracy: 0.5558\n",
      "Epoch 4/10\n",
      "4341/4341 [==============================] - 418s 95ms/step - loss: 0.0789 - accuracy: 0.6046 - val_loss: 0.0891 - val_accuracy: 0.5588\n",
      "Epoch 5/10\n",
      "4341/4341 [==============================] - 418s 96ms/step - loss: 0.0742 - accuracy: 0.6279 - val_loss: 0.0902 - val_accuracy: 0.5363\n",
      "Epoch 6/10\n",
      "4341/4341 [==============================] - 418s 95ms/step - loss: 0.0678 - accuracy: 0.6643 - val_loss: 0.0903 - val_accuracy: 0.5531\n",
      "Epoch 7/10\n",
      "4341/4341 [==============================] - 418s 95ms/step - loss: 0.0567 - accuracy: 0.7196 - val_loss: 0.0967 - val_accuracy: 0.5501\n",
      "Epoch 8/10\n",
      "4341/4341 [==============================] - 419s 96ms/step - loss: 0.0454 - accuracy: 0.7796 - val_loss: 0.1044 - val_accuracy: 0.5470\n",
      "Epoch 9/10\n",
      "4341/4341 [==============================] - 419s 96ms/step - loss: 0.0351 - accuracy: 0.8267 - val_loss: 0.1112 - val_accuracy: 0.5439\n",
      "Epoch 10/10\n",
      "4341/4341 [==============================] - 419s 96ms/step - loss: 0.0274 - accuracy: 0.8625 - val_loss: 0.1152 - val_accuracy: 0.5391\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 69.88304936\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=10)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4597a",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cd521f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e081bfd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c81b7dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:3527 | loss:0.84 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99])\n",
      "\n",
      "----------\n",
      "id:2859 | loss:0.83 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:367 | loss:0.76 | true:[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99])\n",
      "\n",
      "----------\n",
      "id:1399 | loss:0.76 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ded71",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca183ba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c00ef",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 10, epochs = 10, maxlen=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd1f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a4c05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_1 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-2')\n",
    "roberta_predictor_1.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d9feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_1.predict(X_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbda36c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3536970e-03, 9.2545198e-03, 4.2936229e-03, ..., 4.7977617e-01,\n",
       "        1.4663014e-03, 2.7643377e-03],\n",
       "       [9.4114935e-01, 4.5977072e-03, 3.1906173e-03, ..., 2.6671041e-03,\n",
       "        6.3332159e-04, 4.5039970e-03],\n",
       "       [6.5519378e-02, 4.1260761e-03, 1.1885009e-03, ..., 7.0284860e-04,\n",
       "        1.0977413e-02, 3.0193960e-03],\n",
       "       ...,\n",
       "       [5.4345466e-04, 4.9529137e-04, 9.6089346e-04, ..., 8.4625959e-04,\n",
       "        1.5569483e-04, 9.9685174e-01],\n",
       "       [7.6016629e-01, 1.5074391e-03, 5.4397836e-04, ..., 1.0225406e-03,\n",
       "        4.0254142e-04, 2.4462803e-03],\n",
       "       [4.0434580e-04, 7.2131009e-04, 1.4746425e-04, ..., 1.0098093e-01,\n",
       "        5.4532109e-04, 3.2762828e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba_1 = get_proba(test_predictions_1)\n",
    "test_proba_1 = np.array(test_proba_1)\n",
    "test_proba_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7f0281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_1 = proba_to_labels(test_proba_1)\n",
    "y_pred_labels_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "400a3b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.58  0.63\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.59    0.33  0.43\n",
       "annoyance            0.44    0.21  0.28\n",
       "approval             0.42    0.26  0.32\n",
       "caring               0.54    0.27  0.36\n",
       "confusion            0.49    0.28  0.36\n",
       "curiosity            0.53    0.30  0.39\n",
       "desire               0.63    0.35  0.45\n",
       "disappointment       0.49    0.17  0.25\n",
       "disapproval          0.46    0.28  0.35\n",
       "disgust              0.62    0.36  0.45\n",
       "embarrassment        0.59    0.35  0.44\n",
       "excitement           0.65    0.34  0.45\n",
       "fear                 0.65    0.54  0.59\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.43  0.53\n",
       "love                 0.82    0.76  0.79\n",
       "nervousness          0.60    0.13  0.21\n",
       "optimism             0.57    0.37  0.45\n",
       "pride                0.50    0.06  0.11\n",
       "realization          0.34    0.14  0.20\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.65    0.39  0.49\n",
       "sadness              0.71    0.48  0.57\n",
       "surprise             0.67    0.45  0.54\n",
       "neutral              0.69    0.51  0.59\n",
       "MACRO-AVERAGE        0.56    0.36  0.43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_1, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924d332c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3edfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.66    0.67  0.67\n",
       "amusement            0.76    0.80  0.78\n",
       "anger                0.51    0.35  0.41\n",
       "annoyance            0.37    0.27  0.31\n",
       "approval             0.43    0.37  0.40\n",
       "caring               0.51    0.37  0.43\n",
       "confusion            0.40    0.40  0.40\n",
       "curiosity            0.48    0.40  0.44\n",
       "desire               0.58    0.35  0.44\n",
       "disappointment       0.39    0.24  0.30\n",
       "disapproval          0.38    0.28  0.32\n",
       "disgust              0.51    0.41  0.46\n",
       "embarrassment        0.50    0.38  0.43\n",
       "excitement           0.52    0.34  0.41\n",
       "fear                 0.66    0.62  0.64\n",
       "gratitude            0.94    0.90  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.64    0.50  0.56\n",
       "love                 0.77    0.80  0.78\n",
       "nervousness          0.43    0.26  0.32\n",
       "optimism             0.54    0.43  0.48\n",
       "pride                0.75    0.19  0.30\n",
       "realization          0.24    0.15  0.19\n",
       "relief               0.80    0.36  0.50\n",
       "remorse              0.61    0.55  0.58\n",
       "sadness              0.66    0.54  0.60\n",
       "surprise             0.58    0.44  0.50\n",
       "neutral              0.67    0.55  0.61\n",
       "MACRO-AVERAGE        0.58    0.43  0.48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca724679",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_1.predict(X_test.to_list())\n",
    "val_predictions_1 = roberta_predictor_1.predict(X_val.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21992e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c5920c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0d12131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.58  0.63\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.59    0.33  0.43\n",
       "annoyance            0.44    0.21  0.28\n",
       "approval             0.42    0.26  0.32\n",
       "caring               0.54    0.27  0.36\n",
       "confusion            0.49    0.28  0.36\n",
       "curiosity            0.53    0.30  0.39\n",
       "desire               0.63    0.35  0.45\n",
       "disappointment       0.49    0.17  0.25\n",
       "disapproval          0.46    0.28  0.35\n",
       "disgust              0.62    0.36  0.45\n",
       "embarrassment        0.59    0.35  0.44\n",
       "excitement           0.65    0.34  0.45\n",
       "fear                 0.65    0.54  0.59\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.43  0.53\n",
       "love                 0.82    0.76  0.79\n",
       "nervousness          0.60    0.13  0.21\n",
       "optimism             0.57    0.37  0.45\n",
       "pride                0.50    0.06  0.11\n",
       "realization          0.34    0.14  0.20\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.65    0.39  0.49\n",
       "sadness              0.71    0.48  0.57\n",
       "surprise             0.67    0.45  0.54\n",
       "neutral              0.69    0.51  0.59\n",
       "MACRO-AVERAGE        0.56    0.36  0.43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_test_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5d82c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.75    0.66  0.70\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.65    0.41  0.50\n",
       "annoyance            0.42    0.17  0.25\n",
       "approval             0.44    0.26  0.33\n",
       "caring               0.53    0.31  0.39\n",
       "confusion            0.53    0.32  0.40\n",
       "curiosity            0.55    0.32  0.41\n",
       "desire               0.73    0.42  0.53\n",
       "disappointment       0.58    0.23  0.33\n",
       "disapproval          0.44    0.23  0.31\n",
       "disgust              0.46    0.31  0.37\n",
       "embarrassment        0.59    0.37  0.46\n",
       "excitement           0.53    0.21  0.30\n",
       "fear                 0.80    0.43  0.56\n",
       "gratitude            0.93    0.87  0.90\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.38  0.49\n",
       "love                 0.75    0.74  0.75\n",
       "nervousness          0.38    0.14  0.21\n",
       "optimism             0.67    0.42  0.52\n",
       "pride                1.00    0.20  0.33\n",
       "realization          0.38    0.17  0.23\n",
       "relief               1.00    0.06  0.11\n",
       "remorse              0.85    0.32  0.47\n",
       "sadness              0.64    0.42  0.51\n",
       "surprise             0.57    0.43  0.49\n",
       "neutral              0.69    0.50  0.58\n",
       "MACRO-AVERAGE        0.62    0.36  0.43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val, y_pred_labels_val_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2df7a7",
   "metadata": {},
   "source": [
    "## Model 3 - Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a05a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7076111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1695ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65c2e7b",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5115fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4ddc2",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f439c46d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587705b9",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebc4f60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215c3050",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3171bb1",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52db035",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a04c38",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fca183",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b45c1",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3baf37",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907aca58",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e89863",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/15\n",
      "2714/2714 [==============================] - 381s 135ms/step - loss: 0.1574 - accuracy: 0.3877 - val_loss: 0.1031 - val_accuracy: 0.5328\n",
      "Epoch 2/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0962 - accuracy: 0.5427 - val_loss: 0.0895 - val_accuracy: 0.5614\n",
      "Epoch 3/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0849 - accuracy: 0.5819 - val_loss: 0.0857 - val_accuracy: 0.5654\n",
      "Epoch 4/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0781 - accuracy: 0.6106 - val_loss: 0.0875 - val_accuracy: 0.5564\n",
      "Epoch 5/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0720 - accuracy: 0.6400 - val_loss: 0.0878 - val_accuracy: 0.5595\n",
      "Epoch 6/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0667 - accuracy: 0.6711 - val_loss: 0.0919 - val_accuracy: 0.5363\n",
      "Epoch 7/15\n",
      "2714/2714 [==============================] - 372s 136ms/step - loss: 0.0616 - accuracy: 0.6972 - val_loss: 0.0917 - val_accuracy: 0.5523\n",
      "Epoch 8/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0571 - accuracy: 0.7203 - val_loss: 0.0993 - val_accuracy: 0.5282\n",
      "Epoch 9/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0496 - accuracy: 0.7590 - val_loss: 0.1014 - val_accuracy: 0.5383\n",
      "Epoch 10/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0408 - accuracy: 0.8041 - val_loss: 0.1093 - val_accuracy: 0.5394\n",
      "Epoch 11/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0330 - accuracy: 0.8407 - val_loss: 0.1162 - val_accuracy: 0.5356\n",
      "Epoch 12/15\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0262 - accuracy: 0.8650 - val_loss: 0.1245 - val_accuracy: 0.5313\n",
      "Epoch 13/15\n",
      "2714/2714 [==============================] - 369s 135ms/step - loss: 0.0204 - accuracy: 0.8855 - val_loss: 0.1310 - val_accuracy: 0.5243\n",
      "Epoch 14/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0156 - accuracy: 0.8965 - val_loss: 0.1353 - val_accuracy: 0.5440\n",
      "Epoch 15/15\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0124 - accuracy: 0.9050 - val_loss: 0.1411 - val_accuracy: 0.5374\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 92.81196866833334\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=15)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfb74d",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788819f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce3853b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23da5585",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:3527 | loss:0.92 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:4880 | loss:0.89 | true:[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:601 | loss:0.88 | true:[0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.01 0.   0.   0.01 0.   0.   0.   0.02 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99])\n",
      "\n",
      "----------\n",
      "id:367 | loss:0.86 | true:[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77a183",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "105d0923",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243aa96d",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e7445fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813eca1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_3 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-3')\n",
    "roberta_predictor_3.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f136eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_3 = roberta_predictor_3.predict(X_test.to_list())\n",
    "val_predictions_3 = roberta_predictor_3.predict(X_val.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1524feed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.6364049e-03, 1.6514740e-03, 1.1147998e-03, ..., 3.5009545e-01,\n",
       "        9.1472518e-04, 4.7874972e-03],\n",
       "       [9.6141309e-01, 8.4600262e-03, 7.5814466e-04, ..., 5.9351407e-04,\n",
       "        7.8705931e-04, 1.4756176e-03],\n",
       "       [1.2052274e-01, 2.3990225e-03, 8.7369600e-04, ..., 3.9959120e-04,\n",
       "        6.5930746e-03, 5.7433435e-04],\n",
       "       ...,\n",
       "       [4.6241930e-04, 5.0163601e-04, 5.6524662e-04, ..., 2.9905813e-04,\n",
       "        8.4501931e-05, 9.9906605e-01],\n",
       "       [9.6358943e-01, 2.3675426e-03, 4.0471909e-04, ..., 2.9843237e-04,\n",
       "        1.1355062e-03, 9.4567408e-04],\n",
       "       [2.3129837e-04, 2.1638017e-04, 1.0896058e-04, ..., 5.6223869e-03,\n",
       "        2.7416201e-04, 9.9856961e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba_3 = get_proba(test_predictions_3)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "test_proba_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfdfb713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f8d242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.66    0.67  0.67\n",
       "amusement            0.76    0.80  0.78\n",
       "anger                0.51    0.35  0.41\n",
       "annoyance            0.37    0.27  0.31\n",
       "approval             0.43    0.37  0.40\n",
       "caring               0.51    0.37  0.43\n",
       "confusion            0.40    0.40  0.40\n",
       "curiosity            0.48    0.40  0.44\n",
       "desire               0.58    0.35  0.44\n",
       "disappointment       0.39    0.24  0.30\n",
       "disapproval          0.38    0.28  0.32\n",
       "disgust              0.51    0.41  0.46\n",
       "embarrassment        0.50    0.38  0.43\n",
       "excitement           0.52    0.34  0.41\n",
       "fear                 0.66    0.62  0.64\n",
       "gratitude            0.94    0.90  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.64    0.50  0.56\n",
       "love                 0.77    0.80  0.78\n",
       "nervousness          0.43    0.26  0.32\n",
       "optimism             0.54    0.43  0.48\n",
       "pride                0.75    0.19  0.30\n",
       "realization          0.24    0.15  0.19\n",
       "relief               0.80    0.36  0.50\n",
       "remorse              0.61    0.55  0.58\n",
       "sadness              0.66    0.54  0.60\n",
       "surprise             0.58    0.44  0.50\n",
       "neutral              0.67    0.55  0.61\n",
       "MACRO-AVERAGE        0.58    0.43  0.48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b756972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_3)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_3)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149c5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7266cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.66    0.67  0.67\n",
       "amusement            0.76    0.80  0.78\n",
       "anger                0.51    0.35  0.41\n",
       "annoyance            0.37    0.27  0.31\n",
       "approval             0.43    0.37  0.40\n",
       "caring               0.51    0.37  0.43\n",
       "confusion            0.40    0.40  0.40\n",
       "curiosity            0.48    0.40  0.44\n",
       "desire               0.58    0.35  0.44\n",
       "disappointment       0.39    0.24  0.30\n",
       "disapproval          0.38    0.28  0.32\n",
       "disgust              0.51    0.41  0.46\n",
       "embarrassment        0.50    0.38  0.43\n",
       "excitement           0.52    0.34  0.41\n",
       "fear                 0.66    0.62  0.64\n",
       "gratitude            0.94    0.90  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.64    0.50  0.56\n",
       "love                 0.77    0.80  0.78\n",
       "nervousness          0.43    0.26  0.32\n",
       "optimism             0.54    0.43  0.48\n",
       "pride                0.75    0.19  0.30\n",
       "realization          0.24    0.15  0.19\n",
       "relief               0.80    0.36  0.50\n",
       "remorse              0.61    0.55  0.58\n",
       "sadness              0.66    0.54  0.60\n",
       "surprise             0.58    0.44  0.50\n",
       "neutral              0.67    0.55  0.61\n",
       "MACRO-AVERAGE        0.58    0.43  0.48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_test_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f32df245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.68  0.69\n",
       "amusement            0.79    0.79  0.79\n",
       "anger                0.56    0.43  0.49\n",
       "annoyance            0.36    0.28  0.32\n",
       "approval             0.43    0.29  0.35\n",
       "caring               0.51    0.34  0.41\n",
       "confusion            0.47    0.36  0.41\n",
       "curiosity            0.52    0.43  0.47\n",
       "desire               0.54    0.42  0.47\n",
       "disappointment       0.26    0.15  0.19\n",
       "disapproval          0.43    0.29  0.35\n",
       "disgust              0.47    0.38  0.42\n",
       "embarrassment        0.56    0.40  0.47\n",
       "excitement           0.43    0.30  0.35\n",
       "fear                 0.72    0.47  0.57\n",
       "gratitude            0.91    0.87  0.89\n",
       "grief                0.50    0.08  0.13\n",
       "joy                  0.61    0.43  0.50\n",
       "love                 0.72    0.78  0.75\n",
       "nervousness          0.50    0.19  0.28\n",
       "optimism             0.59    0.48  0.53\n",
       "pride                0.80    0.27  0.40\n",
       "realization          0.29    0.19  0.23\n",
       "relief               0.40    0.11  0.17\n",
       "remorse              0.69    0.43  0.53\n",
       "sadness              0.48    0.40  0.44\n",
       "surprise             0.53    0.51  0.52\n",
       "neutral              0.66    0.56  0.61\n",
       "MACRO-AVERAGE        0.55    0.40  0.45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val, y_pred_labels_val_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a80ae",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c4bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d9704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d10a19a",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27032a27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4910e",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961cd106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01621fc",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b25704",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97b5b0e",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200f0a2",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92cc9b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad3741",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55601da4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab8cb6",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4ef50",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a3fc3",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3baccb7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/20\n",
      "2714/2714 [==============================] - 381s 135ms/step - loss: 0.1597 - accuracy: 0.3808 - val_loss: 0.1053 - val_accuracy: 0.5181\n",
      "Epoch 2/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0970 - accuracy: 0.5436 - val_loss: 0.0915 - val_accuracy: 0.5573\n",
      "Epoch 3/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0847 - accuracy: 0.5849 - val_loss: 0.0859 - val_accuracy: 0.5649\n",
      "Epoch 4/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0782 - accuracy: 0.6116 - val_loss: 0.0860 - val_accuracy: 0.5754\n",
      "Epoch 5/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0718 - accuracy: 0.6404 - val_loss: 0.0880 - val_accuracy: 0.5593\n",
      "Epoch 6/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0659 - accuracy: 0.6741 - val_loss: 0.0903 - val_accuracy: 0.5643\n",
      "Epoch 7/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0603 - accuracy: 0.7040 - val_loss: 0.0930 - val_accuracy: 0.5522\n",
      "Epoch 8/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0557 - accuracy: 0.7287 - val_loss: 0.0991 - val_accuracy: 0.5223\n",
      "Epoch 9/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0516 - accuracy: 0.7519 - val_loss: 0.1035 - val_accuracy: 0.5313\n",
      "Epoch 10/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0487 - accuracy: 0.7628 - val_loss: 0.1037 - val_accuracy: 0.5284\n",
      "Epoch 11/20\n",
      "2714/2714 [==============================] - 370s 135ms/step - loss: 0.0446 - accuracy: 0.7839 - val_loss: 0.1138 - val_accuracy: 0.5009\n",
      "Epoch 12/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0377 - accuracy: 0.8191 - val_loss: 0.1147 - val_accuracy: 0.5295\n",
      "Epoch 13/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0312 - accuracy: 0.8469 - val_loss: 0.1237 - val_accuracy: 0.5241\n",
      "Epoch 14/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0259 - accuracy: 0.8680 - val_loss: 0.1282 - val_accuracy: 0.5133\n",
      "Epoch 15/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0207 - accuracy: 0.8850 - val_loss: 0.1399 - val_accuracy: 0.5182\n",
      "Epoch 16/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0167 - accuracy: 0.8964 - val_loss: 0.1413 - val_accuracy: 0.5273\n",
      "Epoch 17/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0132 - accuracy: 0.9047 - val_loss: 0.1509 - val_accuracy: 0.5334\n",
      "Epoch 18/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0102 - accuracy: 0.9066 - val_loss: 0.1570 - val_accuracy: 0.5304\n",
      "Epoch 19/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0076 - accuracy: 0.9125 - val_loss: 0.1636 - val_accuracy: 0.5269\n",
      "Epoch 20/20\n",
      "2714/2714 [==============================] - 371s 135ms/step - loss: 0.0057 - accuracy: 0.9133 - val_loss: 0.1662 - val_accuracy: 0.5335\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 123.69208704166665\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=20)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1aa76",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a74dd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bec05dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67768b02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:4880 | loss:0.97 | true:[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:367 | loss:0.96 | true:[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:979 | loss:0.94 | true:[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01 0.\n",
      " 0.   0.   0.   0.14 0.   0.   0.   0.9  0.03 0.04 0.   0.   0.   0.03])\n",
      "\n",
      "----------\n",
      "id:3201 | loss:0.94 | true:[0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.96 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8 ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2d337",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2155fa36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be714b68",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 20, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d321de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13fb7279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-4')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f62703",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_4 = roberta_predictor_4.predict(X_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18cbbb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.4290747e-03, 7.8636119e-03, 5.1054446e-04, ..., 9.5501281e-02,\n",
       "        7.2957424e-04, 7.3105283e-03],\n",
       "       [8.2433641e-01, 1.9720383e-03, 4.2170432e-04, ..., 5.4175570e-04,\n",
       "        1.7669980e-04, 1.7695238e-04],\n",
       "       [1.3946671e-02, 2.2126360e-04, 1.9208476e-03, ..., 1.7582820e-04,\n",
       "        3.9282176e-03, 4.8753218e-04],\n",
       "       ...,\n",
       "       [1.1307964e-04, 9.4718311e-04, 9.8380806e-05, ..., 9.4783976e-04,\n",
       "        1.2550938e-04, 9.9937904e-01],\n",
       "       [9.8604071e-01, 8.5227825e-03, 4.0846289e-04, ..., 4.7407774e-04,\n",
       "        2.6980783e-03, 1.1469327e-04],\n",
       "       [7.4175063e-05, 2.0628533e-04, 4.7285434e-05, ..., 8.6860557e-04,\n",
       "        1.7152395e-04, 9.9964035e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba_4 = get_proba(test_predictions_4)\n",
    "test_proba_4 = np.array(test_proba_4)\n",
    "test_proba_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fbde694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_4 = proba_to_labels(test_proba_4)\n",
    "y_pred_labels_4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6d4e1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.66    0.69  0.67\n",
       "amusement            0.76    0.82  0.79\n",
       "anger                0.49    0.44  0.46\n",
       "annoyance            0.35    0.30  0.32\n",
       "approval             0.37    0.29  0.32\n",
       "caring               0.45    0.39  0.42\n",
       "confusion            0.42    0.40  0.41\n",
       "curiosity            0.47    0.46  0.46\n",
       "desire               0.57    0.35  0.43\n",
       "disappointment       0.36    0.25  0.29\n",
       "disapproval          0.39    0.32  0.35\n",
       "disgust              0.48    0.40  0.43\n",
       "embarrassment        0.57    0.32  0.41\n",
       "excitement           0.54    0.37  0.44\n",
       "fear                 0.67    0.67  0.67\n",
       "gratitude            0.93    0.89  0.91\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.62    0.53  0.57\n",
       "love                 0.75    0.80  0.77\n",
       "nervousness          0.40    0.43  0.42\n",
       "optimism             0.59    0.46  0.51\n",
       "pride                0.40    0.12  0.19\n",
       "realization          0.32    0.19  0.24\n",
       "relief               0.50    0.45  0.48\n",
       "remorse              0.53    0.54  0.53\n",
       "sadness              0.62    0.53  0.57\n",
       "surprise             0.57    0.47  0.52\n",
       "neutral              0.64    0.59  0.62\n",
       "MACRO-AVERAGE        0.51    0.44  0.47"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_4, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5a950",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42293da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f3bd79",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6286aa36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=48, classes=GE_taxonomy, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b5bff",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96b3a841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34787809",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f0e3d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85751c8b",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138a6ed",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111e452e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09831db",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd48d4c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8c68d",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde55542",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce7744",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0875ed1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/12\n",
      "2714/2714 [==============================] - 338s 120ms/step - loss: 0.1579 - accuracy: 0.3827 - val_loss: 0.1041 - val_accuracy: 0.5149\n",
      "Epoch 2/12\n",
      "2714/2714 [==============================] - 332s 121ms/step - loss: 0.0970 - accuracy: 0.5365 - val_loss: 0.0902 - val_accuracy: 0.5538\n",
      "Epoch 3/12\n",
      "2714/2714 [==============================] - 331s 121ms/step - loss: 0.0857 - accuracy: 0.5788 - val_loss: 0.0873 - val_accuracy: 0.5638\n",
      "Epoch 4/12\n",
      "2714/2714 [==============================] - 331s 121ms/step - loss: 0.0790 - accuracy: 0.6099 - val_loss: 0.0858 - val_accuracy: 0.5732\n",
      "Epoch 5/12\n",
      "2714/2714 [==============================] - 333s 121ms/step - loss: 0.0735 - accuracy: 0.6357 - val_loss: 0.0887 - val_accuracy: 0.5483\n",
      "Epoch 6/12\n",
      "2714/2714 [==============================] - 334s 122ms/step - loss: 0.0679 - accuracy: 0.6658 - val_loss: 0.0898 - val_accuracy: 0.5514\n",
      "Epoch 7/12\n",
      "2714/2714 [==============================] - 333s 121ms/step - loss: 0.0608 - accuracy: 0.7032 - val_loss: 0.0945 - val_accuracy: 0.5439\n",
      "Epoch 8/12\n",
      "2714/2714 [==============================] - 331s 121ms/step - loss: 0.0511 - accuracy: 0.7535 - val_loss: 0.1000 - val_accuracy: 0.5439\n",
      "Epoch 9/12\n",
      "2714/2714 [==============================] - 335s 122ms/step - loss: 0.0411 - accuracy: 0.8030 - val_loss: 0.1068 - val_accuracy: 0.5311\n",
      "Epoch 10/12\n",
      "2714/2714 [==============================] - 338s 123ms/step - loss: 0.0325 - accuracy: 0.8417 - val_loss: 0.1156 - val_accuracy: 0.5302\n",
      "Epoch 11/12\n",
      "2714/2714 [==============================] - 337s 123ms/step - loss: 0.0253 - accuracy: 0.8690 - val_loss: 0.1211 - val_accuracy: 0.5280\n",
      "Epoch 12/12\n",
      "2714/2714 [==============================] - 332s 121ms/step - loss: 0.0201 - accuracy: 0.8879 - val_loss: 0.1250 - val_accuracy: 0.5345\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 66.71130312833333\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=12)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ab4c8",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad85e387",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "474fabd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed09b110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:1433 | loss:0.91 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99])\n",
      "\n",
      "----------\n",
      "id:4880 | loss:0.83 | true:[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:450 | loss:0.78 | true:[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n",
      "----------\n",
      "id:2300 | loss:0.78 | true:[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33562180",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71a8e728",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985106f",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 10, epochs = 10, maxlen=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62393553",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-5.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "661788ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_1 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-5.1')\n",
    "roberta_predictor_1.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e89be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_1.predict(X_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8cd3513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3536970e-03, 9.2545198e-03, 4.2936229e-03, ..., 4.7977617e-01,\n",
       "        1.4663014e-03, 2.7643377e-03],\n",
       "       [9.4114935e-01, 4.5977072e-03, 3.1906173e-03, ..., 2.6671041e-03,\n",
       "        6.3332159e-04, 4.5039970e-03],\n",
       "       [6.5519378e-02, 4.1260761e-03, 1.1885009e-03, ..., 7.0284860e-04,\n",
       "        1.0977413e-02, 3.0193960e-03],\n",
       "       ...,\n",
       "       [5.4345466e-04, 4.9529137e-04, 9.6089346e-04, ..., 8.4625959e-04,\n",
       "        1.5569483e-04, 9.9685174e-01],\n",
       "       [7.6016629e-01, 1.5074391e-03, 5.4397836e-04, ..., 1.0225406e-03,\n",
       "        4.0254142e-04, 2.4462803e-03],\n",
       "       [4.0434580e-04, 7.2131009e-04, 1.4746425e-04, ..., 1.0098093e-01,\n",
       "        5.4532109e-04, 3.2762828e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proba_1 = get_proba(test_predictions_1)\n",
    "test_proba_1 = np.array(test_proba_1)\n",
    "test_proba_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "874c8ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_1 = proba_to_labels(test_proba_1)\n",
    "y_pred_labels_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "704c7008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.58  0.63\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.59    0.33  0.43\n",
       "annoyance            0.44    0.21  0.28\n",
       "approval             0.42    0.26  0.32\n",
       "caring               0.54    0.27  0.36\n",
       "confusion            0.49    0.28  0.36\n",
       "curiosity            0.53    0.30  0.39\n",
       "desire               0.63    0.35  0.45\n",
       "disappointment       0.49    0.17  0.25\n",
       "disapproval          0.46    0.28  0.35\n",
       "disgust              0.62    0.36  0.45\n",
       "embarrassment        0.59    0.35  0.44\n",
       "excitement           0.65    0.34  0.45\n",
       "fear                 0.65    0.54  0.59\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.43  0.53\n",
       "love                 0.82    0.76  0.79\n",
       "nervousness          0.60    0.13  0.21\n",
       "optimism             0.57    0.37  0.45\n",
       "pride                0.50    0.06  0.11\n",
       "realization          0.34    0.14  0.20\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.65    0.39  0.49\n",
       "sadness              0.71    0.48  0.57\n",
       "surprise             0.67    0.45  0.54\n",
       "neutral              0.69    0.51  0.59\n",
       "MACRO-AVERAGE        0.56    0.36  0.43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_1, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3893c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e497e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.66    0.67  0.67\n",
       "amusement            0.76    0.80  0.78\n",
       "anger                0.51    0.35  0.41\n",
       "annoyance            0.37    0.27  0.31\n",
       "approval             0.43    0.37  0.40\n",
       "caring               0.51    0.37  0.43\n",
       "confusion            0.40    0.40  0.40\n",
       "curiosity            0.48    0.40  0.44\n",
       "desire               0.58    0.35  0.44\n",
       "disappointment       0.39    0.24  0.30\n",
       "disapproval          0.38    0.28  0.32\n",
       "disgust              0.51    0.41  0.46\n",
       "embarrassment        0.50    0.38  0.43\n",
       "excitement           0.52    0.34  0.41\n",
       "fear                 0.66    0.62  0.64\n",
       "gratitude            0.94    0.90  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.64    0.50  0.56\n",
       "love                 0.77    0.80  0.78\n",
       "nervousness          0.43    0.26  0.32\n",
       "optimism             0.54    0.43  0.48\n",
       "pride                0.75    0.19  0.30\n",
       "realization          0.24    0.15  0.19\n",
       "relief               0.80    0.36  0.50\n",
       "remorse              0.61    0.55  0.58\n",
       "sadness              0.66    0.54  0.60\n",
       "surprise             0.58    0.44  0.50\n",
       "neutral              0.67    0.55  0.61\n",
       "MACRO-AVERAGE        0.58    0.43  0.48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376a9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_1.predict(X_test.to_list())\n",
    "val_predictions_1 = roberta_predictor_1.predict(X_val.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb442a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33aec54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a00c3bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.58  0.63\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.59    0.33  0.43\n",
       "annoyance            0.44    0.21  0.28\n",
       "approval             0.42    0.26  0.32\n",
       "caring               0.54    0.27  0.36\n",
       "confusion            0.49    0.28  0.36\n",
       "curiosity            0.53    0.30  0.39\n",
       "desire               0.63    0.35  0.45\n",
       "disappointment       0.49    0.17  0.25\n",
       "disapproval          0.46    0.28  0.35\n",
       "disgust              0.62    0.36  0.45\n",
       "embarrassment        0.59    0.35  0.44\n",
       "excitement           0.65    0.34  0.45\n",
       "fear                 0.65    0.54  0.59\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.43  0.53\n",
       "love                 0.82    0.76  0.79\n",
       "nervousness          0.60    0.13  0.21\n",
       "optimism             0.57    0.37  0.45\n",
       "pride                0.50    0.06  0.11\n",
       "realization          0.34    0.14  0.20\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.65    0.39  0.49\n",
       "sadness              0.71    0.48  0.57\n",
       "surprise             0.67    0.45  0.54\n",
       "neutral              0.69    0.51  0.59\n",
       "MACRO-AVERAGE        0.56    0.36  0.43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_test_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "798d98e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.75    0.66  0.70\n",
       "amusement            0.81    0.73  0.77\n",
       "anger                0.65    0.41  0.50\n",
       "annoyance            0.42    0.17  0.25\n",
       "approval             0.44    0.26  0.33\n",
       "caring               0.53    0.31  0.39\n",
       "confusion            0.53    0.32  0.40\n",
       "curiosity            0.55    0.32  0.41\n",
       "desire               0.73    0.42  0.53\n",
       "disappointment       0.58    0.23  0.33\n",
       "disapproval          0.44    0.23  0.31\n",
       "disgust              0.46    0.31  0.37\n",
       "embarrassment        0.59    0.37  0.46\n",
       "excitement           0.53    0.21  0.30\n",
       "fear                 0.80    0.43  0.56\n",
       "gratitude            0.93    0.87  0.90\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.68    0.38  0.49\n",
       "love                 0.75    0.74  0.75\n",
       "nervousness          0.38    0.14  0.21\n",
       "optimism             0.67    0.42  0.52\n",
       "pride                1.00    0.20  0.33\n",
       "realization          0.38    0.17  0.23\n",
       "relief               1.00    0.06  0.11\n",
       "remorse              0.85    0.32  0.47\n",
       "sadness              0.64    0.42  0.51\n",
       "surprise             0.57    0.43  0.49\n",
       "neutral              0.69    0.50  0.58\n",
       "MACRO-AVERAGE        0.62    0.36  0.43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val, y_pred_labels_val_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587e887",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b89b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3c28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a36094b",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfa618da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=48, classes=GE_taxonomy, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816af7e5",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97321898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train.to_list(), y_train)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val.to_list(), y_val)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test.to_list(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a38d9",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6ed80e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7ec4a8c",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95626d4a",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd917323",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  612124    \n",
      "=================================================================\n",
      "Total params: 124,667,164\n",
      "Trainable params: 124,667,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056384b",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec99dea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e382a38aeeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrate_finder_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroberta_learner_ins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTotal time in minutes on estimating optimal learning rate: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrate_finder_stop_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrate_finder_start_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361af8d",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b48e9",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4e11c",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97879e5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/5\n",
      "2714/2714 [==============================] - 340s 121ms/step - loss: 0.1723 - accuracy: 0.3602 - val_loss: 0.1065 - val_accuracy: 0.4994\n",
      "Epoch 2/5\n",
      "2714/2714 [==============================] - 331s 121ms/step - loss: 0.0978 - accuracy: 0.5325 - val_loss: 0.0893 - val_accuracy: 0.5640\n",
      "Epoch 3/5\n",
      "2714/2714 [==============================] - 331s 121ms/step - loss: 0.0853 - accuracy: 0.5798 - val_loss: 0.0838 - val_accuracy: 0.5805\n",
      "Epoch 4/5\n",
      "2714/2714 [==============================] - 332s 121ms/step - loss: 0.0741 - accuracy: 0.6301 - val_loss: 0.0834 - val_accuracy: 0.5684\n",
      "Epoch 5/5\n",
      "2714/2714 [==============================] - 332s 121ms/step - loss: 0.0630 - accuracy: 0.6888 - val_loss: 0.0853 - val_accuracy: 0.5700\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 27.764415828333345\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=3e-5, epochs=5)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5d733",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ed309c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8ab6584",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dcfd0d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:4880 | loss:0.59 | true:[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.01 0.01 0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.96])\n",
      "\n",
      "----------\n",
      "id:1433 | loss:0.57 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.01 0.04 0.02 0.02 0.   0.   0.   0.01 0.02 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.01 0.   0.02 0.   0.   0.01 0.   0.88])\n",
      "\n",
      "----------\n",
      "id:3482 | loss:0.57 | true:[0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.01 0.01 0.01 0.02 0.02 0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.94])\n",
      "\n",
      "----------\n",
      "id:3527 | loss:0.55 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] | pred:[0.   0.   0.   0.02 0.01 0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.01 0.   0.01 0.   0.   0.   0.   0.93])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da5048",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa5484f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608c73b",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 10, epochs = 10, maxlen=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae423858",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-6.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "989f3c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_1 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-6.1')\n",
    "roberta_predictor_1.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f03f7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_1.predict(X_test.to_list())\n",
    "val_predictions_1 = roberta_predictor_1.predict(X_val.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1eff7e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7160b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bddec6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.76    0.56  0.64\n",
       "amusement            0.84    0.62  0.72\n",
       "anger                0.74    0.16  0.27\n",
       "annoyance            0.86    0.04  0.07\n",
       "approval             0.76    0.18  0.29\n",
       "caring               0.67    0.21  0.33\n",
       "confusion            0.62    0.20  0.30\n",
       "curiosity            0.68    0.09  0.16\n",
       "desire               0.70    0.17  0.27\n",
       "disappointment       0.86    0.04  0.08\n",
       "disapproval          0.57    0.12  0.20\n",
       "disgust              0.83    0.20  0.33\n",
       "embarrassment        0.57    0.11  0.18\n",
       "excitement           0.82    0.22  0.35\n",
       "fear                 0.82    0.42  0.56\n",
       "gratitude            0.97    0.88  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.80    0.27  0.40\n",
       "love                 0.85    0.69  0.76\n",
       "nervousness          0.00    0.00  0.00\n",
       "optimism             0.72    0.31  0.44\n",
       "pride                0.00    0.00  0.00\n",
       "realization          0.75    0.06  0.11\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.75    0.05  0.10\n",
       "sadness              0.81    0.38  0.52\n",
       "surprise             0.73    0.31  0.44\n",
       "neutral              0.80    0.33  0.47\n",
       "MACRO-AVERAGE        0.65    0.24  0.32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test, y_pred_labels_test_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21d67f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.80    0.58  0.67\n",
       "amusement            0.85    0.63  0.72\n",
       "anger                0.85    0.24  0.37\n",
       "annoyance            0.82    0.03  0.06\n",
       "approval             0.69    0.14  0.23\n",
       "caring               0.73    0.24  0.36\n",
       "confusion            0.63    0.22  0.33\n",
       "curiosity            0.78    0.12  0.20\n",
       "desire               0.84    0.27  0.41\n",
       "disappointment       1.00    0.01  0.01\n",
       "disapproval          0.63    0.08  0.15\n",
       "disgust              0.71    0.21  0.32\n",
       "embarrassment        0.90    0.26  0.40\n",
       "excitement           0.76    0.14  0.23\n",
       "fear                 0.94    0.37  0.53\n",
       "gratitude            0.96    0.87  0.91\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.87    0.26  0.40\n",
       "love                 0.78    0.69  0.73\n",
       "nervousness          0.00    0.00  0.00\n",
       "optimism             0.76    0.34  0.47\n",
       "pride                0.00    0.00  0.00\n",
       "realization          0.92    0.09  0.17\n",
       "relief               0.00    0.00  0.00\n",
       "remorse              0.33    0.01  0.03\n",
       "sadness              0.67    0.36  0.47\n",
       "surprise             0.63    0.30  0.41\n",
       "neutral              0.80    0.33  0.47\n",
       "MACRO-AVERAGE        0.67    0.24  0.32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val, y_pred_labels_val_3, GE_taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1383ff7",
   "metadata": {},
   "source": [
    "### Go Emotion without neutral emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b959e",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b83a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56dfae00",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4a0fa",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b510066a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53329ebe",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3865fe22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829b75e",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eca4541",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40fe2f00",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbc843",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48dfa9ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb14e5a",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accb45c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbb90b",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be2f25",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7dbc9",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ae8b82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/15\n",
      "1912/1912 [==============================] - 273s 136ms/step - loss: 0.1889 - accuracy: 0.2382 - val_loss: 0.1209 - val_accuracy: 0.4760\n",
      "Epoch 2/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.1060 - accuracy: 0.5428 - val_loss: 0.0964 - val_accuracy: 0.5743\n",
      "Epoch 3/15\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0893 - accuracy: 0.5969 - val_loss: 0.0889 - val_accuracy: 0.5965\n",
      "Epoch 4/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0802 - accuracy: 0.6348 - val_loss: 0.0897 - val_accuracy: 0.5829\n",
      "Epoch 5/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0736 - accuracy: 0.6644 - val_loss: 0.0889 - val_accuracy: 0.5856\n",
      "Epoch 6/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0675 - accuracy: 0.6891 - val_loss: 0.0901 - val_accuracy: 0.5902\n",
      "Epoch 7/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0627 - accuracy: 0.7154 - val_loss: 0.0935 - val_accuracy: 0.5756\n",
      "Epoch 8/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0579 - accuracy: 0.7343 - val_loss: 0.0981 - val_accuracy: 0.5712\n",
      "Epoch 9/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0502 - accuracy: 0.7689 - val_loss: 0.0999 - val_accuracy: 0.5722\n",
      "Epoch 10/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0420 - accuracy: 0.8045 - val_loss: 0.1031 - val_accuracy: 0.5816\n",
      "Epoch 11/15\n",
      "1912/1912 [==============================] - 263s 136ms/step - loss: 0.0338 - accuracy: 0.8372 - val_loss: 0.1118 - val_accuracy: 0.5751\n",
      "Epoch 12/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0267 - accuracy: 0.8579 - val_loss: 0.1173 - val_accuracy: 0.5845\n",
      "Epoch 13/15\n",
      "1912/1912 [==============================] - 263s 136ms/step - loss: 0.0207 - accuracy: 0.8787 - val_loss: 0.1259 - val_accuracy: 0.5762\n",
      "Epoch 14/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0159 - accuracy: 0.8880 - val_loss: 0.1292 - val_accuracy: 0.5759\n",
      "Epoch 15/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0125 - accuracy: 0.8911 - val_loss: 0.1321 - val_accuracy: 0.5769\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 65.69253288166666\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=15)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcea203",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d49cf8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ebafb67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f0a9cc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:1042 | loss:0.85 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.99 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:2959 | loss:0.8 | true:[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:1527 | loss:0.74 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.   1.   0.03 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:2512 | loss:0.74 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.01 0.   0.   0.   0.   0.09 0.03 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.75])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10748567",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3719b2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467e82c",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77d81ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5235cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-1')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10512a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25a92673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03d318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18f73635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.73    0.71  0.72\n",
       "amusement            0.84    0.85  0.84\n",
       "anger                0.59    0.43  0.50\n",
       "annoyance            0.48    0.37  0.42\n",
       "approval             0.51    0.46  0.48\n",
       "caring               0.52    0.47  0.49\n",
       "confusion            0.51    0.48  0.49\n",
       "curiosity            0.67    0.59  0.63\n",
       "desire               0.67    0.45  0.54\n",
       "disappointment       0.43    0.32  0.37\n",
       "disapproval          0.55    0.51  0.53\n",
       "disgust              0.58    0.42  0.49\n",
       "embarrassment        0.54    0.35  0.43\n",
       "excitement           0.62    0.39  0.48\n",
       "fear                 0.68    0.64  0.66\n",
       "gratitude            0.95    0.90  0.92\n",
       "grief                0.50    0.17  0.25\n",
       "joy                  0.68    0.52  0.59\n",
       "love                 0.81    0.85  0.83\n",
       "nervousness          0.62    0.22  0.32\n",
       "optimism             0.63    0.49  0.55\n",
       "pride                0.50    0.12  0.20\n",
       "realization          0.39    0.23  0.29\n",
       "relief               0.67    0.36  0.47\n",
       "remorse              0.67    0.57  0.62\n",
       "sadness              0.66    0.49  0.56\n",
       "surprise             0.66    0.47  0.55\n",
       "MACRO-AVERAGE        0.62    0.48  0.53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10614681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.73    0.74  0.73\n",
       "amusement            0.85    0.79  0.82\n",
       "anger                0.60    0.45  0.51\n",
       "annoyance            0.46    0.38  0.41\n",
       "approval             0.54    0.45  0.49\n",
       "caring               0.59    0.50  0.54\n",
       "confusion            0.53    0.42  0.47\n",
       "curiosity            0.72    0.61  0.66\n",
       "desire               0.58    0.45  0.51\n",
       "disappointment       0.48    0.31  0.37\n",
       "disapproval          0.59    0.45  0.51\n",
       "disgust              0.51    0.39  0.44\n",
       "embarrassment        0.59    0.49  0.53\n",
       "excitement           0.39    0.27  0.32\n",
       "fear                 0.79    0.50  0.61\n",
       "gratitude            0.92    0.89  0.90\n",
       "grief                0.67    0.15  0.25\n",
       "joy                  0.63    0.47  0.54\n",
       "love                 0.76    0.79  0.78\n",
       "nervousness          0.38    0.14  0.21\n",
       "optimism             0.67    0.52  0.59\n",
       "pride                1.00    0.20  0.33\n",
       "realization          0.40    0.28  0.33\n",
       "relief               0.33    0.17  0.22\n",
       "remorse              0.72    0.38  0.50\n",
       "sadness              0.61    0.49  0.54\n",
       "surprise             0.63    0.53  0.58\n",
       "MACRO-AVERAGE        0.62    0.45  0.51"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932c181",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c46086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa3e6ec4",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5833a",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d160ed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dd5e5",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217aaadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ea08a",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacc2532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca8bcdd",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892d96",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b293d9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb33c9",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1403c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c1a19",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93df2b",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86aa29",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43232c10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/20\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0119 - accuracy: 0.8935 - val_loss: 0.1352 - val_accuracy: 0.5762\n",
      "Epoch 2/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0118 - accuracy: 0.8914 - val_loss: 0.1401 - val_accuracy: 0.5691\n",
      "Epoch 3/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0122 - accuracy: 0.8905 - val_loss: 0.1463 - val_accuracy: 0.5738\n",
      "Epoch 4/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0130 - accuracy: 0.8899 - val_loss: 0.1454 - val_accuracy: 0.5623\n",
      "Epoch 5/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0141 - accuracy: 0.8873 - val_loss: 0.1482 - val_accuracy: 0.5644\n",
      "Epoch 6/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0152 - accuracy: 0.8826 - val_loss: 0.1488 - val_accuracy: 0.5626\n",
      "Epoch 7/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0155 - accuracy: 0.8817 - val_loss: 0.1488 - val_accuracy: 0.5678\n",
      "Epoch 8/20\n",
      "1912/1912 [==============================] - 263s 136ms/step - loss: 0.0176 - accuracy: 0.8742 - val_loss: 0.1491 - val_accuracy: 0.5704\n",
      "Epoch 9/20\n",
      "1912/1912 [==============================] - 264s 137ms/step - loss: 0.0184 - accuracy: 0.8710 - val_loss: 0.1482 - val_accuracy: 0.5519\n",
      "Epoch 10/20\n",
      "1912/1912 [==============================] - 264s 137ms/step - loss: 0.0194 - accuracy: 0.8710 - val_loss: 0.1469 - val_accuracy: 0.5683\n",
      "Epoch 11/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0187 - accuracy: 0.8718 - val_loss: 0.1526 - val_accuracy: 0.5490\n",
      "Epoch 12/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0155 - accuracy: 0.8791 - val_loss: 0.1542 - val_accuracy: 0.5629\n",
      "Epoch 13/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0126 - accuracy: 0.8860 - val_loss: 0.1599 - val_accuracy: 0.5550\n",
      "Epoch 14/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0103 - accuracy: 0.8883 - val_loss: 0.1628 - val_accuracy: 0.5613\n",
      "Epoch 15/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0080 - accuracy: 0.8893 - val_loss: 0.1702 - val_accuracy: 0.5665\n",
      "Epoch 16/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0058 - accuracy: 0.8940 - val_loss: 0.1778 - val_accuracy: 0.5660\n",
      "Epoch 17/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0044 - accuracy: 0.8995 - val_loss: 0.1798 - val_accuracy: 0.5842\n",
      "Epoch 18/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0032 - accuracy: 0.8961 - val_loss: 0.1847 - val_accuracy: 0.5790\n",
      "Epoch 19/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0024 - accuracy: 0.8974 - val_loss: 0.1873 - val_accuracy: 0.5759\n",
      "Epoch 20/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0017 - accuracy: 0.8978 - val_loss: 0.1892 - val_accuracy: 0.5788\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 87.25766121833334\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=20)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe99df",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b1e09c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "445c7c01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98eaf937",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:1042 | loss:1.33 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n",
      "----------\n",
      "id:2959 | loss:1.25 | true:[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n",
      "----------\n",
      "id:1661 | loss:1.06 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n",
      "----------\n",
      "id:481 | loss:1.05 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cb84c",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b04824aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebc7c9",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d822398",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d76503f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-2')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f9cac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26b0d731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55ce2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c469f83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.74  0.72\n",
       "amusement            0.82    0.89  0.85\n",
       "anger                0.59    0.49  0.54\n",
       "annoyance            0.47    0.38  0.42\n",
       "approval             0.54    0.45  0.49\n",
       "caring               0.52    0.45  0.48\n",
       "confusion            0.51    0.46  0.48\n",
       "curiosity            0.69    0.63  0.66\n",
       "desire               0.71    0.47  0.57\n",
       "disappointment       0.42    0.29  0.34\n",
       "disapproval          0.56    0.51  0.54\n",
       "disgust              0.56    0.43  0.49\n",
       "embarrassment        0.48    0.35  0.41\n",
       "excitement           0.53    0.37  0.43\n",
       "fear                 0.69    0.72  0.70\n",
       "gratitude            0.94    0.89  0.92\n",
       "grief                0.20    0.17  0.18\n",
       "joy                  0.65    0.57  0.61\n",
       "love                 0.78    0.85  0.82\n",
       "nervousness          0.39    0.30  0.34\n",
       "optimism             0.65    0.53  0.58\n",
       "pride                0.67    0.25  0.36\n",
       "realization          0.42    0.28  0.33\n",
       "relief               0.45    0.45  0.45\n",
       "remorse              0.65    0.71  0.68\n",
       "sadness              0.63    0.54  0.58\n",
       "surprise             0.60    0.48  0.53\n",
       "MACRO-AVERAGE        0.59    0.51  0.54"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc9f94f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.70    0.74  0.72\n",
       "amusement            0.83    0.83  0.83\n",
       "anger                0.64    0.49  0.56\n",
       "annoyance            0.42    0.35  0.38\n",
       "approval             0.56    0.45  0.50\n",
       "caring               0.61    0.49  0.55\n",
       "confusion            0.59    0.42  0.49\n",
       "curiosity            0.65    0.64  0.64\n",
       "desire               0.61    0.56  0.59\n",
       "disappointment       0.45    0.31  0.36\n",
       "disapproval          0.56    0.45  0.50\n",
       "disgust              0.47    0.41  0.44\n",
       "embarrassment        0.61    0.54  0.58\n",
       "excitement           0.42    0.29  0.35\n",
       "fear                 0.79    0.54  0.64\n",
       "gratitude            0.94    0.89  0.92\n",
       "grief                0.56    0.38  0.45\n",
       "joy                  0.66    0.52  0.58\n",
       "love                 0.76    0.84  0.80\n",
       "nervousness          0.56    0.24  0.33\n",
       "optimism             0.70    0.54  0.61\n",
       "pride                0.60    0.20  0.30\n",
       "realization          0.35    0.27  0.30\n",
       "relief               0.40    0.22  0.29\n",
       "remorse              0.70    0.56  0.62\n",
       "sadness              0.56    0.52  0.54\n",
       "surprise             0.55    0.55  0.55\n",
       "MACRO-AVERAGE        0.60    0.49  0.53"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991425eb",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdbf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3243bc4",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bdfa8",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d00c0426",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e020e8f",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "691f64c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe621d6",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae428886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cd6d2c4",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaccb88",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e162b8fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44de47",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296ed8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cd259",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75eb02",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19df619",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c229b9c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 5e-05...\n",
      "Epoch 1/10\n",
      "1912/1912 [==============================] - 271s 136ms/step - loss: 0.1831 - accuracy: 0.2761 - val_loss: 0.1150 - val_accuracy: 0.5143\n",
      "Epoch 2/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.1028 - accuracy: 0.5485 - val_loss: 0.0910 - val_accuracy: 0.5955\n",
      "Epoch 3/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0873 - accuracy: 0.6041 - val_loss: 0.0880 - val_accuracy: 0.5926\n",
      "Epoch 4/10\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0795 - accuracy: 0.6341 - val_loss: 0.0898 - val_accuracy: 0.5827\n",
      "Epoch 5/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0739 - accuracy: 0.6600 - val_loss: 0.0894 - val_accuracy: 0.5832\n",
      "Epoch 6/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0659 - accuracy: 0.6959 - val_loss: 0.0918 - val_accuracy: 0.5707\n",
      "Epoch 7/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0549 - accuracy: 0.7459 - val_loss: 0.0940 - val_accuracy: 0.5848\n",
      "Epoch 8/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0441 - accuracy: 0.7967 - val_loss: 0.0988 - val_accuracy: 0.5720\n",
      "Epoch 9/10\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0350 - accuracy: 0.8340 - val_loss: 0.1042 - val_accuracy: 0.5824\n",
      "Epoch 10/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0274 - accuracy: 0.8603 - val_loss: 0.1074 - val_accuracy: 0.5822\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 43.73559640500001\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=5e-5, epochs=10)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ca3b4",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79adb744",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89bfd441",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eed6e0f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:814 | loss:0.68 | true:[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0.] | pred:[0.35 0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.86 0.   0.06 0.   0.   0.   0.   0.   0.02 0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:395 | loss:0.68 | true:[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1.] | pred:[0.   0.   0.   0.   0.99 0.   0.01 0.1  0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:2713 | loss:0.63 | true:[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.01 0.01 0.   0.01 0.99 0.   0.   0.   0.   0.   0.01\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01])\n",
      "\n",
      "----------\n",
      "id:1527 | loss:0.62 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.01 0.98 0.   0.   0.   0.   0.01 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.01 0.   0.01 0.   0.   0.   0.  ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dba27a",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48dc3a91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7b291",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95bf0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f99c5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-3')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea961c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6549dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e71c5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "693d9d5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.75    0.70  0.72\n",
       "amusement            0.85    0.78  0.81\n",
       "anger                0.65    0.40  0.50\n",
       "annoyance            0.52    0.28  0.37\n",
       "approval             0.55    0.43  0.49\n",
       "caring               0.62    0.37  0.47\n",
       "confusion            0.59    0.42  0.49\n",
       "curiosity            0.74    0.53  0.62\n",
       "desire               0.74    0.37  0.50\n",
       "disappointment       0.45    0.21  0.29\n",
       "disapproval          0.62    0.42  0.50\n",
       "disgust              0.67    0.34  0.45\n",
       "embarrassment        0.68    0.35  0.46\n",
       "excitement           0.71    0.38  0.49\n",
       "fear                 0.69    0.63  0.66\n",
       "gratitude            0.96    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.76    0.44  0.56\n",
       "love                 0.83    0.76  0.79\n",
       "nervousness          0.67    0.26  0.38\n",
       "optimism             0.72    0.47  0.57\n",
       "pride                0.33    0.06  0.11\n",
       "realization          0.40    0.18  0.25\n",
       "relief               0.67    0.18  0.29\n",
       "remorse              0.67    0.46  0.55\n",
       "sadness              0.75    0.53  0.62\n",
       "surprise             0.70    0.44  0.54\n",
       "MACRO-AVERAGE        0.64    0.42  0.50"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d10c5ae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.76    0.70  0.73\n",
       "amusement            0.87    0.74  0.80\n",
       "anger                0.66    0.41  0.50\n",
       "annoyance            0.47    0.27  0.35\n",
       "approval             0.61    0.37  0.46\n",
       "caring               0.68    0.46  0.55\n",
       "confusion            0.59    0.40  0.48\n",
       "curiosity            0.74    0.49  0.59\n",
       "desire               0.67    0.43  0.52\n",
       "disappointment       0.54    0.28  0.37\n",
       "disapproval          0.63    0.39  0.48\n",
       "disgust              0.57    0.35  0.43\n",
       "embarrassment        0.77    0.49  0.60\n",
       "excitement           0.52    0.29  0.37\n",
       "fear                 0.77    0.53  0.63\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.69    0.40  0.50\n",
       "love                 0.80    0.74  0.77\n",
       "nervousness          0.75    0.14  0.24\n",
       "optimism             0.68    0.46  0.55\n",
       "pride                1.00    0.13  0.24\n",
       "realization          0.43    0.26  0.32\n",
       "relief               0.60    0.17  0.26\n",
       "remorse              0.76    0.24  0.36\n",
       "sadness              0.65    0.48  0.55\n",
       "surprise             0.61    0.44  0.51\n",
       "MACRO-AVERAGE        0.66    0.41  0.49"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb6497",
   "metadata": {},
   "source": [
    "## Model 4 - Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55e98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9a2b97",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ba4e2",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d289869",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d2a24",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b7e0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e643c2",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0c4336",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee93ca34",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f7d89",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43deae33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb961793",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ade57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c13509",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fd8b8",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45282e",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24c741d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/10\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0251 - accuracy: 0.8671 - val_loss: 0.1109 - val_accuracy: 0.5746\n",
      "Epoch 2/10\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0245 - accuracy: 0.8663 - val_loss: 0.1162 - val_accuracy: 0.5816\n",
      "Epoch 3/10\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0238 - accuracy: 0.8691 - val_loss: 0.1214 - val_accuracy: 0.5647\n",
      "Epoch 4/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0239 - accuracy: 0.8658 - val_loss: 0.1219 - val_accuracy: 0.5647\n",
      "Epoch 5/10\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0242 - accuracy: 0.8666 - val_loss: 0.1256 - val_accuracy: 0.5582\n",
      "Epoch 6/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0222 - accuracy: 0.8696 - val_loss: 0.1319 - val_accuracy: 0.5566\n",
      "Epoch 7/10\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0177 - accuracy: 0.8815 - val_loss: 0.1358 - val_accuracy: 0.5639\n",
      "Epoch 8/10\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0133 - accuracy: 0.8907 - val_loss: 0.1397 - val_accuracy: 0.5681\n",
      "Epoch 9/10\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0101 - accuracy: 0.8918 - val_loss: 0.1426 - val_accuracy: 0.5775\n",
      "Epoch 10/10\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0073 - accuracy: 0.8970 - val_loss: 0.1481 - val_accuracy: 0.5751\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 43.592460323333334\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=3e-5, epochs=10)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ced1f",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30d96a6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d3a8110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c95c080",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:2959 | loss:0.94 | true:[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n",
      "----------\n",
      "id:1510 | loss:0.88 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0.] | pred:[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01])\n",
      "\n",
      "----------\n",
      "id:2713 | loss:0.85 | true:[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n",
      "----------\n",
      "id:9 | loss:0.84 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.   1.   0.01 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad4aa4",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e079f08d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2c514",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8af88be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68566ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-4')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59e209e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0dc74f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "841142e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a470970d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.72    0.73  0.73\n",
       "amusement            0.83    0.83  0.83\n",
       "anger                0.62    0.48  0.54\n",
       "annoyance            0.44    0.35  0.39\n",
       "approval             0.52    0.50  0.51\n",
       "caring               0.54    0.47  0.50\n",
       "confusion            0.52    0.51  0.51\n",
       "curiosity            0.68    0.65  0.66\n",
       "desire               0.74    0.47  0.57\n",
       "disappointment       0.37    0.25  0.29\n",
       "disapproval          0.57    0.50  0.53\n",
       "disgust              0.58    0.48  0.52\n",
       "embarrassment        0.57    0.35  0.43\n",
       "excitement           0.55    0.45  0.49\n",
       "fear                 0.69    0.73  0.71\n",
       "gratitude            0.95    0.90  0.93\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.69    0.54  0.60\n",
       "love                 0.79    0.83  0.81\n",
       "nervousness          0.53    0.39  0.45\n",
       "optimism             0.63    0.51  0.56\n",
       "pride                0.50    0.19  0.27\n",
       "realization          0.39    0.24  0.30\n",
       "relief               0.75    0.55  0.63\n",
       "remorse              0.61    0.71  0.66\n",
       "sadness              0.62    0.57  0.60\n",
       "surprise             0.63    0.50  0.56\n",
       "MACRO-AVERAGE        0.59    0.51  0.54"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56dddd93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.73    0.72  0.73\n",
       "amusement            0.83    0.79  0.81\n",
       "anger                0.63    0.49  0.55\n",
       "annoyance            0.42    0.39  0.41\n",
       "approval             0.51    0.43  0.46\n",
       "caring               0.58    0.47  0.52\n",
       "confusion            0.56    0.48  0.52\n",
       "curiosity            0.66    0.61  0.63\n",
       "desire               0.62    0.51  0.56\n",
       "disappointment       0.43    0.31  0.36\n",
       "disapproval          0.59    0.49  0.54\n",
       "disgust              0.46    0.44  0.45\n",
       "embarrassment        0.71    0.49  0.58\n",
       "excitement           0.41    0.33  0.37\n",
       "fear                 0.76    0.58  0.66\n",
       "gratitude            0.93    0.88  0.91\n",
       "grief                0.67    0.31  0.42\n",
       "joy                  0.67    0.49  0.56\n",
       "love                 0.77    0.83  0.80\n",
       "nervousness          0.64    0.33  0.44\n",
       "optimism             0.65    0.57  0.61\n",
       "pride                0.60    0.20  0.30\n",
       "realization          0.34    0.24  0.28\n",
       "relief               0.29    0.22  0.25\n",
       "remorse              0.72    0.50  0.59\n",
       "sadness              0.61    0.54  0.57\n",
       "surprise             0.56    0.49  0.52\n",
       "MACRO-AVERAGE        0.60    0.49  0.53"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47b668",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4d14a1e",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e8eff",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cc8df0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88934a72",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac1e78cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea6bf4",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47af3265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e64bc961",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626b29c",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f931f4a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6c9ee",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854abbee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b4047",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8cbcf",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581727e",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a54ac95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/15\n",
      "1912/1912 [==============================] - 272s 136ms/step - loss: 0.2135 - accuracy: 0.1947 - val_loss: 0.1387 - val_accuracy: 0.4366\n",
      "Epoch 2/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.1173 - accuracy: 0.5106 - val_loss: 0.0992 - val_accuracy: 0.5811\n",
      "Epoch 3/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0940 - accuracy: 0.5885 - val_loss: 0.0893 - val_accuracy: 0.6035\n",
      "Epoch 4/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0837 - accuracy: 0.6196 - val_loss: 0.0879 - val_accuracy: 0.6012\n",
      "Epoch 5/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0759 - accuracy: 0.6525 - val_loss: 0.0865 - val_accuracy: 0.6075\n",
      "Epoch 6/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0695 - accuracy: 0.6797 - val_loss: 0.0899 - val_accuracy: 0.5874\n",
      "Epoch 7/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0636 - accuracy: 0.7094 - val_loss: 0.0905 - val_accuracy: 0.5962\n",
      "Epoch 8/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0576 - accuracy: 0.7351 - val_loss: 0.0931 - val_accuracy: 0.5947\n",
      "Epoch 9/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0497 - accuracy: 0.7710 - val_loss: 0.0983 - val_accuracy: 0.5772\n",
      "Epoch 10/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0414 - accuracy: 0.8076 - val_loss: 0.1027 - val_accuracy: 0.5785\n",
      "Epoch 11/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0337 - accuracy: 0.8400 - val_loss: 0.1100 - val_accuracy: 0.5696\n",
      "Epoch 12/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0279 - accuracy: 0.8578 - val_loss: 0.1142 - val_accuracy: 0.5709\n",
      "Epoch 13/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0224 - accuracy: 0.8737 - val_loss: 0.1174 - val_accuracy: 0.5728\n",
      "Epoch 14/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0182 - accuracy: 0.8832 - val_loss: 0.1204 - val_accuracy: 0.5793\n",
      "Epoch 15/15\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0153 - accuracy: 0.8884 - val_loss: 0.1232 - val_accuracy: 0.5775\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 65.67875105833333\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=3e-5, epochs=15)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974f6ba",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29d2926a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03bff493",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ccd049b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:814 | loss:0.92 | true:[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0.] | pred:[0.97 0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.29 0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:1661 | loss:0.75 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[1.   0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:475 | loss:0.72 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.01 1.   0.   0.   0.   0.01 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:1527 | loss:0.72 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef31a0a",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76acdb2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d6fe46",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3455628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c1c6465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-5')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "500061df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24559dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2bd2244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66746cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.74    0.71  0.72\n",
       "amusement            0.83    0.81  0.82\n",
       "anger                0.60    0.43  0.50\n",
       "annoyance            0.45    0.34  0.39\n",
       "approval             0.57    0.44  0.50\n",
       "caring               0.62    0.48  0.54\n",
       "confusion            0.51    0.48  0.50\n",
       "curiosity            0.71    0.57  0.63\n",
       "desire               0.72    0.41  0.52\n",
       "disappointment       0.46    0.28  0.35\n",
       "disapproval          0.55    0.47  0.51\n",
       "disgust              0.53    0.39  0.45\n",
       "embarrassment        0.65    0.35  0.46\n",
       "excitement           0.54    0.39  0.45\n",
       "fear                 0.70    0.67  0.68\n",
       "gratitude            0.95    0.89  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.69    0.43  0.53\n",
       "love                 0.81    0.84  0.82\n",
       "nervousness          0.56    0.22  0.31\n",
       "optimism             0.61    0.45  0.52\n",
       "pride                0.50    0.25  0.33\n",
       "realization          0.42    0.19  0.26\n",
       "relief               0.75    0.27  0.40\n",
       "remorse              0.65    0.55  0.60\n",
       "sadness              0.69    0.50  0.58\n",
       "surprise             0.62    0.46  0.53\n",
       "MACRO-AVERAGE        0.65    0.46  0.52"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "deec9849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.76    0.71  0.73\n",
       "amusement            0.82    0.79  0.81\n",
       "anger                0.66    0.47  0.55\n",
       "annoyance            0.47    0.37  0.41\n",
       "approval             0.59    0.41  0.49\n",
       "caring               0.62    0.49  0.55\n",
       "confusion            0.54    0.42  0.47\n",
       "curiosity            0.69    0.60  0.64\n",
       "desire               0.67    0.47  0.55\n",
       "disappointment       0.47    0.28  0.35\n",
       "disapproval          0.59    0.44  0.51\n",
       "disgust              0.49    0.41  0.45\n",
       "embarrassment        0.69    0.51  0.59\n",
       "excitement           0.45    0.26  0.33\n",
       "fear                 0.74    0.53  0.62\n",
       "gratitude            0.95    0.88  0.92\n",
       "grief                0.00    0.00  0.00\n",
       "joy                  0.65    0.40  0.50\n",
       "love                 0.76    0.77  0.77\n",
       "nervousness          0.44    0.19  0.27\n",
       "optimism             0.66    0.50  0.57\n",
       "pride                0.83    0.33  0.48\n",
       "realization          0.39    0.23  0.29\n",
       "relief               0.50    0.17  0.25\n",
       "remorse              0.71    0.35  0.47\n",
       "sadness              0.63    0.47  0.54\n",
       "surprise             0.65    0.56  0.60\n",
       "MACRO-AVERAGE        0.61    0.45  0.51"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8669d1",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43324e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f48bc7",
   "metadata": {},
   "source": [
    "X_train = train_GE[:][\"Clean_text\"]\n",
    "y_train = train_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_train_no_neu = df_train_GE_no_neu[:][\"Clean_text\"]\n",
    "y_train_no_neu = df_train_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_val = val_GE[:][\"Clean_text\"]\n",
    "y_val = val_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_val_no_neu = df_val_GE_no_neu[:][\"Clean_text\"]\n",
    "y_val_no_neu = df_val_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "\n",
    "X_test = test_GE[:][\"Clean_text\"]\n",
    "y_test = test_GE.loc[:, GE_taxonomy].values.astype(float)\n",
    "X_test_no_neu = df_test_GE_no_neu[:][\"Clean_text\"]\n",
    "y_test_no_neu = df_test_GE_no_neu.loc[:, GE_taxonomy_no_neu].values.astype(float)\n",
    "#print(X_train.shape, y_train.shape,y_train_no_neu.shape, X_val.shape, y_val.shape,y_val_no_neu.shape, X_test.shape, y_test.shape, y_test_no_neu.shape)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9a658",
   "metadata": {
    "id": "J72fuWOHCvOv"
   },
   "source": [
    "### Instantiating a RoBERTa Instance:\n",
    "\n",
    "Create a RoBERTa instance with the model name, max token length, the labels to be used for each category and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "993ed856",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "afdd0fe6b4444d3d8fb110e39c5b545a",
      "11d89264c87849668c3389f3abcec402",
      "4f82815745c74cf49a3fd0fefa07a936",
      "a52b6fd89f7745208e45a35b8d5cbb25",
      "b7ad46c64b4f45d9ad42847de6875121",
      "6aa2595042604c72a9ec19542d19dfcc",
      "25a9f06ade054211a66ec39705b0033d",
      "de2873a70866483a8fa25b402d80dfec",
      "78c0e7db41a847f998916481e9e65235",
      "f8dd2a89479d4046a277bbfe6089218c",
      "059aeb8799a6469ca780ddf625b95e8c"
     ]
    },
    "id": "fhnT_kSwCohg",
    "outputId": "a5d0a279-ab39-42e3-acf6-934b4fafd572"
   },
   "outputs": [],
   "source": [
    "roberta_transformer = text.Transformer('roberta-base', maxlen=56, classes=GE_taxonomy_no_neu, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f3feb",
   "metadata": {},
   "source": [
    "### Perform Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccc1a9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 26\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 14\n",
      "\t95percentile : 25\n",
      "\t99percentile : 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_train = roberta_transformer.preprocess_train(X_train_no_neu.to_list(), y_train_no_neu)\n",
    "roberta_val = roberta_transformer.preprocess_test(X_val_no_neu.to_list(), y_val_no_neu)\n",
    "roberta_test = roberta_transformer.preprocess_test(X_test_no_neu.to_list(), y_test_no_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5796376",
   "metadata": {
    "id": "vahQyHQOFBmH"
   },
   "source": [
    "### Compile RoBERTa in a K-Train Learner Object:\n",
    "\n",
    "Since we are using k-train as a high level abstration package, we need to wrap our model in a k-train Learner Object for further compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c262cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d53b674ff45d40aba6475adb74dcb3e0",
      "bda1822cb6f04a34b695b5672d702cf1",
      "78c16388124e4534b689fc7404142e06",
      "5a3d8183fee546e9868c577af6cecbf6",
      "e5cbf541c19844399ff2901ea0f0dbeb",
      "e5fc3c01e5894dc6834c6d0c64deb4aa",
      "e5285948234d43838879edc56a67aff6",
      "5a68ecf5f15040a9b9207e449f90d346",
      "dd65b07b18c04856a012cc4535f8726f",
      "29ec025304354a09b6197600ffb1639d",
      "8f7dc806f3fd48378ce616afc63a29ca"
     ]
    },
    "id": "pz8SyfHnD4jI",
    "outputId": "7945f831-eb31-473f-914a-883d148ddfd7"
   },
   "outputs": [],
   "source": [
    "roberta_model = roberta_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0619d68",
   "metadata": {
    "id": "LhXSMlXvFJIg"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins = ktrain.get_learner(model=roberta_model,\n",
    "                            train_data=roberta_train,\n",
    "                            val_data=roberta_val,\n",
    "                            batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e20f63",
   "metadata": {
    "id": "LwXeiW4ZFRW-"
   },
   "source": [
    "### RoBERTa Model Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bddb07bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8x9clsAD-CQ",
    "outputId": "adf57072-5d80-4a55-aabb-6a243d719aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  611355    \n",
      "=================================================================\n",
      "Total params: 124,666,395\n",
      "Trainable params: 124,666,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a4210",
   "metadata": {
    "id": "qLnpkZLcFbGo"
   },
   "source": [
    "### Find Optimal Learning Rate for RoBERTa:\n",
    "\n",
    "This is an optional step used just to show how the learning rate can be found for any transformer model.\n",
    "For Transformer models as per the research papers, the optimal learning rates have already been estimated and established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecdc2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "muU7v4cjFV0x",
    "outputId": "717af625-75dc-4bb1-d2e4-64c73021ac10"
   },
   "source": [
    "rate_finder_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.lr_find(show_plot=True, max_epochs=3)\n",
    "rate_finder_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes on estimating optimal learning rate: \\n\", (rate_finder_stop_time - rate_finder_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9e606",
   "metadata": {},
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "free_gpu_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8eaf6f",
   "metadata": {
    "id": "TcigfbJlRy4_"
   },
   "source": [
    "### RoBERTa Optimal Learning Rates:\n",
    "\n",
    "As per the evaluations made in the research paper \"**RoBERTa: A Robustly Optimized BERT Pretraining Approach**\", below are the best choices in terms of fine-tuning the model:\n",
    "\n",
    "* Batch Sizes => {16, 32}\n",
    "* Learning Rates => {1e−5, 2e−5, 3e−5}\n",
    "\n",
    "We will choose the maximum among these for our fine-tuning and evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320d7a9",
   "metadata": {
    "id": "LuxErKnFVznL"
   },
   "source": [
    "### Fine Tuning RoBERTa on Emotion Dataset:\n",
    "\n",
    "We take our emotion dataset along with the RoBERTa model, define the learning-rate & epochs to be used and start fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bbd3bc02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02TtNt3xGgyz",
    "outputId": "f62e7b4b-7b07-4f1e-a46f-4ec001aae699",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/20\n",
      "1912/1912 [==============================] - 271s 136ms/step - loss: 0.2196 - accuracy: 0.1577 - val_loss: 0.1467 - val_accuracy: 0.3868\n",
      "Epoch 2/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.1246 - accuracy: 0.4737 - val_loss: 0.1066 - val_accuracy: 0.5480\n",
      "Epoch 3/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0998 - accuracy: 0.5644 - val_loss: 0.0931 - val_accuracy: 0.5887\n",
      "Epoch 4/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0880 - accuracy: 0.6071 - val_loss: 0.0908 - val_accuracy: 0.5996\n",
      "Epoch 5/20\n",
      "1912/1912 [==============================] - 263s 136ms/step - loss: 0.0805 - accuracy: 0.6363 - val_loss: 0.0884 - val_accuracy: 0.5884\n",
      "Epoch 6/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0739 - accuracy: 0.6645 - val_loss: 0.0900 - val_accuracy: 0.5889\n",
      "Epoch 7/20\n",
      "1912/1912 [==============================] - 263s 136ms/step - loss: 0.0680 - accuracy: 0.6922 - val_loss: 0.0911 - val_accuracy: 0.5889\n",
      "Epoch 8/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0618 - accuracy: 0.7194 - val_loss: 0.0931 - val_accuracy: 0.5772\n",
      "Epoch 9/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0567 - accuracy: 0.7434 - val_loss: 0.0959 - val_accuracy: 0.5782\n",
      "Epoch 10/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0520 - accuracy: 0.7633 - val_loss: 0.1001 - val_accuracy: 0.5712\n",
      "Epoch 11/20\n",
      "1912/1912 [==============================] - 262s 135ms/step - loss: 0.0464 - accuracy: 0.7891 - val_loss: 0.1037 - val_accuracy: 0.5681\n",
      "Epoch 12/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0394 - accuracy: 0.8191 - val_loss: 0.1085 - val_accuracy: 0.5722\n",
      "Epoch 13/20\n",
      "1912/1912 [==============================] - 261s 135ms/step - loss: 0.0333 - accuracy: 0.8418 - val_loss: 0.1127 - val_accuracy: 0.5608\n",
      "Epoch 14/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0275 - accuracy: 0.8588 - val_loss: 0.1207 - val_accuracy: 0.5712\n",
      "Epoch 15/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0230 - accuracy: 0.8709 - val_loss: 0.1232 - val_accuracy: 0.5704\n",
      "Epoch 16/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0188 - accuracy: 0.8811 - val_loss: 0.1279 - val_accuracy: 0.5681\n",
      "Epoch 17/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0150 - accuracy: 0.8858 - val_loss: 0.1359 - val_accuracy: 0.5704\n",
      "Epoch 18/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0124 - accuracy: 0.8915 - val_loss: 0.1370 - val_accuracy: 0.5704\n",
      "Epoch 19/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0099 - accuracy: 0.8967 - val_loss: 0.1401 - val_accuracy: 0.5707\n",
      "Epoch 20/20\n",
      "1912/1912 [==============================] - 262s 136ms/step - loss: 0.0083 - accuracy: 0.8987 - val_loss: 0.1421 - val_accuracy: 0.5738\n",
      "\n",
      "Total time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \n",
      " 87.41851243166668\n"
     ]
    }
   ],
   "source": [
    "roberta_fine_tune_start_time = timeit.default_timer()\n",
    "roberta_learner_ins.fit_onecycle(lr=3e-5, epochs=20)\n",
    "roberta_fine_tune_stop_time = timeit.default_timer()\n",
    "\n",
    "print(\"\\nTotal time in minutes for Fine-Tuning RoBERTa on GoEmotion Dataset: \\n\", (roberta_fine_tune_stop_time - roberta_fine_tune_start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111e138",
   "metadata": {
    "id": "uHV1FvhGdmui"
   },
   "source": [
    "### Checking RoBERTa performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e66448a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW72lIpMXVHU",
    "outputId": "f8e73bcc-bfda-42e9-d13e-674283114c75"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1fb9c5a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf-vIGezd7gK",
    "outputId": "14ff373f-079b-4729-dbca-0d7208d1bfc9"
   },
   "outputs": [],
   "source": [
    "roberta_learner_ins.validate(class_names=GE_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "905bb92a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2cRxhb1DfT",
    "outputId": "7609ecdc-5266-4433-8bab-dc7d46ec4810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:2959 | loss:0.8 | true:[1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.99 0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:2022 | loss:0.8 | true:[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.] | pred:[0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:1661 | loss:0.79 | true:[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0.] | pred:[1.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n",
      "----------\n",
      "id:520 | loss:0.78 | true:[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0.] | pred:[0.   0.   0.   0.01 0.   0.   1.   0.01 0.   0.01 0.01 0.   0.   0.\n",
      " 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_learner_ins.view_top_losses(preproc=roberta_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094ed85",
   "metadata": {
    "id": "vSccv20spzkY"
   },
   "source": [
    "### Saving RoBERTa Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11ead854",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6v0gN8tpzRa",
    "outputId": "07314709-5fb9-4d6e-b8af-3facf107b87c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor = ktrain.get_predictor(roberta_learner_ins.model, preproc=roberta_transformer)\n",
    "roberta_predictor.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175b451",
   "metadata": {},
   "source": [
    "### LR - 5e-5, batch size = 16, epochs = 15, maxlen=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39c0bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_predictor.save('roberta-emotion-predictor-goemotion-no-neu-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08957347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admiration',\n",
       " 'amusement',\n",
       " 'anger',\n",
       " 'annoyance',\n",
       " 'approval',\n",
       " 'caring',\n",
       " 'confusion',\n",
       " 'curiosity',\n",
       " 'desire',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disgust',\n",
       " 'embarrassment',\n",
       " 'excitement',\n",
       " 'fear',\n",
       " 'gratitude',\n",
       " 'grief',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'nervousness',\n",
       " 'optimism',\n",
       " 'pride',\n",
       " 'realization',\n",
       " 'relief',\n",
       " 'remorse',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_predictor_4 = ktrain.load_predictor('roberta-emotion-predictor-goemotion-no-neu-6')\n",
    "roberta_predictor_4.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b8863ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_1 = roberta_predictor_4.predict(X_test_no_neu.to_list())\n",
    "val_predictions_1 = roberta_predictor_4.predict(X_val_no_neu.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2e9bbf8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_proba_3 = get_proba(test_predictions_1)\n",
    "test_proba_3 = np.array(test_proba_3)\n",
    "\n",
    "val_proba_3 = get_proba(val_predictions_1)\n",
    "val_proba_3 = np.array(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6539150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "y_pred_labels_test_3 = proba_to_labels(test_proba_3)\n",
    "y_pred_labels_val_3 = proba_to_labels(val_proba_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2820bdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.71    0.73  0.72\n",
       "amusement            0.82    0.87  0.85\n",
       "anger                0.54    0.44  0.48\n",
       "annoyance            0.47    0.35  0.40\n",
       "approval             0.50    0.47  0.49\n",
       "caring               0.56    0.47  0.51\n",
       "confusion            0.51    0.52  0.51\n",
       "curiosity            0.65    0.60  0.62\n",
       "desire               0.70    0.51  0.59\n",
       "disappointment       0.45    0.27  0.34\n",
       "disapproval          0.52    0.51  0.51\n",
       "disgust              0.57    0.44  0.50\n",
       "embarrassment        0.50    0.38  0.43\n",
       "excitement           0.58    0.40  0.47\n",
       "fear                 0.69    0.69  0.69\n",
       "gratitude            0.93    0.90  0.92\n",
       "grief                1.00    0.17  0.29\n",
       "joy                  0.69    0.52  0.59\n",
       "love                 0.80    0.82  0.80\n",
       "nervousness          0.47    0.30  0.37\n",
       "optimism             0.64    0.52  0.57\n",
       "pride                0.50    0.12  0.20\n",
       "realization          0.35    0.19  0.25\n",
       "relief               0.25    0.09  0.13\n",
       "remorse              0.60    0.54  0.57\n",
       "sadness              0.64    0.55  0.59\n",
       "surprise             0.61    0.48  0.54\n",
       "MACRO-AVERAGE        0.60    0.48  0.52"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_test_no_neu, y_pred_labels_test_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e5b59ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACRO-AVERAGE</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Precision  Recall    F1\n",
       "admiration           0.72    0.73  0.73\n",
       "amusement            0.81    0.80  0.81\n",
       "anger                0.56    0.47  0.51\n",
       "annoyance            0.45    0.36  0.40\n",
       "approval             0.52    0.43  0.47\n",
       "caring               0.59    0.50  0.54\n",
       "confusion            0.55    0.48  0.51\n",
       "curiosity            0.69    0.65  0.67\n",
       "desire               0.60    0.48  0.53\n",
       "disappointment       0.46    0.28  0.35\n",
       "disapproval          0.59    0.47  0.52\n",
       "disgust              0.47    0.44  0.46\n",
       "embarrassment        0.73    0.54  0.62\n",
       "excitement           0.46    0.32  0.38\n",
       "fear                 0.83    0.58  0.68\n",
       "gratitude            0.94    0.89  0.91\n",
       "grief                0.50    0.08  0.13\n",
       "joy                  0.63    0.47  0.54\n",
       "love                 0.78    0.79  0.78\n",
       "nervousness          0.57    0.19  0.29\n",
       "optimism             0.66    0.56  0.61\n",
       "pride                0.50    0.13  0.21\n",
       "realization          0.30    0.24  0.27\n",
       "relief               0.43    0.17  0.24\n",
       "remorse              0.69    0.43  0.53\n",
       "sadness              0.61    0.59  0.60\n",
       "surprise             0.59    0.54  0.57\n",
       "MACRO-AVERAGE        0.60    0.47  0.51"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model_eval(y_val_no_neu, y_pred_labels_val_3, GE_taxonomy_no_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabd3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
